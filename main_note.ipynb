{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy.spatial import distance\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "import sklearn.metrics\n",
    "import scipy.io as sio\n",
    "\n",
    "import ABIDEParser as Reader\n",
    "import train_GCN as Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fold(train_ind, test_ind, val_ind, graph_feat, features, y, y_data, params, subject_IDs):\n",
    "    \"\"\"\n",
    "        train_ind       : indices of the training samples\n",
    "        test_ind        : indices of the test samples\n",
    "        val_ind         : indices of the validation samples\n",
    "        graph_feat      : population graph computed from phenotypic measures num_subjects x num_subjects\n",
    "        features        : feature vectors num_subjects x num_features\n",
    "        y               : ground truth labels (num_subjects x 1)\n",
    "        y_data          : ground truth labels - different representation (num_subjects x 2)\n",
    "        params          : dictionnary of GCNs parameters\n",
    "        subject_IDs     : list of subject IDs\n",
    "\n",
    "    returns:\n",
    "\n",
    "        test_acc    : average accuracy over the test samples using GCNs\n",
    "        test_auc    : average area under curve over the test samples using GCNs\n",
    "        lin_acc     : average accuracy over the test samples using the linear classifier\n",
    "        lin_auc     : average area under curve over the test samples using the linear classifier\n",
    "        fold_size   : number of test samples\n",
    "    \"\"\"\n",
    "\n",
    "    print(len(train_ind))\n",
    "\n",
    "    # selection of a subset of data if running experiments with a subset of the training set\n",
    "    labeled_ind = Reader.site_percentage(train_ind, params['num_training'], subject_IDs)\n",
    "\n",
    "    # feature selection/dimensionality reduction step\n",
    "    x_data = Reader.feature_selection(features, y, labeled_ind, params['num_features'])\n",
    "\n",
    "    fold_size = len(test_ind)\n",
    "\n",
    "    # Calculate all pairwise distances\n",
    "    distv = distance.pdist(x_data, metric='correlation')\n",
    "    # Convert to a square symmetric distance matrix\n",
    "    dist = distance.squareform(distv)\n",
    "    sigma = np.mean(dist)\n",
    "    # Get affinity from similarity matrix\n",
    "    sparse_graph = np.exp(- dist ** 2 / (2 * sigma ** 2))\n",
    "    final_graph = graph_feat * sparse_graph\n",
    "\n",
    "    # Linear classifier\n",
    "    clf = RidgeClassifier()\n",
    "    clf.fit(x_data[train_ind, :], y[train_ind].ravel())\n",
    "    # Compute the accuracy\n",
    "    lin_acc = clf.score(x_data[test_ind, :], y[test_ind].ravel())\n",
    "    # Compute the AUC\n",
    "    pred = clf.decision_function(x_data[test_ind, :])\n",
    "    lin_auc = sklearn.metrics.roc_auc_score(y[test_ind] - 1, pred)\n",
    "\n",
    "    print(\"Linear Accuracy: \" + str(lin_acc))\n",
    "\n",
    "    # Classification with GCNs\n",
    "    test_acc, test_auc = Train.run_training(final_graph, sparse.coo_matrix(x_data).tolil(), y_data, train_ind, val_ind,\n",
    "                                            test_ind, params)\n",
    "\n",
    "    print(test_acc)\n",
    "\n",
    "    # return number of correctly classified samples instead of percentage\n",
    "    test_acc = int(round(test_acc * len(test_ind)))\n",
    "    lin_acc = int(round(lin_acc * len(test_ind)))\n",
    "\n",
    "    return test_acc, test_auc, lin_acc, lin_auc, fold_size\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description='Graph CNNs for population graphs: '\n",
    "                                                 'classification of the ABIDE dataset')\n",
    "    parser.add_argument('--dropout', default=0.3, type=float,\n",
    "                        help='Dropout rate (1 - keep probability) (default: 0.3)')\n",
    "    parser.add_argument('--decay', default=5e-4, type=float,\n",
    "                        help='Weight for L2 loss on embedding matrix (default: 5e-4)')\n",
    "    parser.add_argument('--hidden', default=16, type=int, help='Number of filters in hidden layers (default: 16)')\n",
    "    parser.add_argument('--lrate', default=0.005, type=float, help='Initial learning rate (default: 0.005)')\n",
    "    parser.add_argument('--atlas', default='ho', help='atlas for network construction (node definition) (default: ho, '\n",
    "                                                      'see preprocessed-connectomes-project.org/abide/Pipelines.html '\n",
    "                                                      'for more options )')\n",
    "    parser.add_argument('--epochs', default=150, type=int, help='Number of epochs to train')\n",
    "    parser.add_argument('--num_features', default=2000, type=int, help='Number of features to keep for '\n",
    "                                                                       'the feature selection step (default: 2000)')\n",
    "    parser.add_argument('--num_training', default=1.0, type=float, help='Percentage of training set used for '\n",
    "                                                                        'training (default: 1.0)')\n",
    "    parser.add_argument('--depth', default=0, type=int, help='Number of additional hidden layers in the GCN. '\n",
    "                                                             'Total number of hidden layers: 1+depth (default: 0)')\n",
    "    parser.add_argument('--model', default='gcn_cheby', help='gcn model used (default: gcn_cheby, '\n",
    "                                                             'uses chebyshev polynomials, '\n",
    "                                                             'options: gcn, gcn_cheby, dense )')\n",
    "    parser.add_argument('--seed', default=123, type=int, help='Seed for random initialisation (default: 123)')\n",
    "    parser.add_argument('--folds', default=11, type=int, help='For cross validation, specifies which fold will be '\n",
    "                                                             'used. All folds are used if set to 11 (default: 11)')\n",
    "    parser.add_argument('--save', default=1, type=int, help='Parameter that specifies if results have to be saved. '\n",
    "                                                            'Results will be saved if set to 1 (default: 1)')\n",
    "    parser.add_argument('--connectivity', default='correlation', help='Type of connectivity used for network '\n",
    "                                                                      'construction (default: correlation, '\n",
    "                                                                      'options: correlation, partial correlation, '\n",
    "                                                                      'tangent)')\n",
    "\n",
    "    args = parser.parse_args([])\n",
    "    start_time = time.time()\n",
    "\n",
    "    # GCN Parameters\n",
    "    params = dict()\n",
    "    params['model'] = args.model                    # gcn model using chebyshev polynomials\n",
    "    params['lrate'] = args.lrate                    # Initial learning rate\n",
    "    params['epochs'] = args.epochs                  # Number of epochs to train\n",
    "    params['dropout'] = args.dropout                # Dropout rate (1 - keep probability)\n",
    "    params['hidden'] = args.hidden                  # Number of units in hidden layers\n",
    "    params['decay'] = args.decay                    # Weight for L2 loss on embedding matrix.\n",
    "    params['early_stopping'] = params['epochs']     # Tolerance for early stopping (# of epochs). No early stopping if set to param.epochs\n",
    "    params['max_degree'] = 3                        # Maximum Chebyshev polynomial degree.\n",
    "    params['depth'] = args.depth                    # number of additional hidden layers in the GCN. Total number of hidden layers: 1+depth\n",
    "    params['seed'] = args.seed                      # seed for random initialisation\n",
    "\n",
    "    # GCN Parameters\n",
    "    params['num_features'] = args.num_features      # number of features for feature selection step\n",
    "    params['num_training'] = args.num_training      # percentage of training set used for training\n",
    "    atlas = args.atlas                              # atlas for network construction (node definition)\n",
    "    connectivity = args.connectivity                # type of connectivity used for network construction\n",
    "\n",
    "    # Get class labels\n",
    "    subject_IDs = Reader.get_ids()\n",
    "    labels = Reader.get_subject_score(subject_IDs, score='DX_GROUP')\n",
    "\n",
    "    # Get acquisition site\n",
    "    sites = Reader.get_subject_score(subject_IDs, score='SITE_ID')\n",
    "    unique = np.unique(list(sites.values())).tolist()\n",
    "\n",
    "    num_classes = 2\n",
    "    num_nodes = len(subject_IDs)\n",
    "\n",
    "    # Initialise variables for class labels and acquisition sites\n",
    "    y_data = np.zeros([num_nodes, num_classes])\n",
    "    y = np.zeros([num_nodes, 1])\n",
    "    site = np.zeros([num_nodes, 1], dtype=int)\n",
    "\n",
    "    # Get class labels and acquisition site for all subjects\n",
    "    for i in range(num_nodes):\n",
    "        y_data[i, int(labels[subject_IDs[i]])-1] = 1\n",
    "        y[i] = int(labels[subject_IDs[i]])\n",
    "        site[i] = unique.index(sites[subject_IDs[i]])\n",
    "\n",
    "    # Compute feature vectors (vectorised connectivity networks)\n",
    "    features = Reader.get_networks(subject_IDs, kind=connectivity, atlas_name=atlas)\n",
    "\n",
    "    # Compute population graph using gender and acquisition site\n",
    "    graph = Reader.create_affinity_graph_from_scores(['SEX', 'SITE_ID'], subject_IDs)\n",
    "\n",
    "    # Folds for cross validation experiments\n",
    "    skf = StratifiedKFold(n_splits=10)\n",
    "\n",
    "    if args.folds == 11:  # run cross validation on all folds\n",
    "        scores = Parallel(n_jobs=10)(delayed(train_fold)(train_ind, test_ind, test_ind, graph, features, y, y_data,\n",
    "                                                         params, subject_IDs)\n",
    "                                     for train_ind, test_ind in\n",
    "                                     reversed(list(skf.split(np.zeros(num_nodes), np.squeeze(y)))))\n",
    "\n",
    "        print(scores)\n",
    "\n",
    "        scores_acc = [x[0] for x in scores]\n",
    "        scores_auc = [x[1] for x in scores]\n",
    "        scores_lin = [x[2] for x in scores]\n",
    "        scores_auc_lin = [x[3] for x in scores]\n",
    "        fold_size = [x[4] for x in scores]\n",
    "\n",
    "        print('overall linear accuracy %f' + str(np.sum(scores_lin) * 1. / num_nodes))\n",
    "        print('overall linear AUC %f' + str(np.mean(scores_auc_lin)))\n",
    "        print('overall accuracy %f' + str(np.sum(scores_acc) * 1. / num_nodes))\n",
    "        print('overall AUC %f' + str(np.mean(scores_auc)))\n",
    "\n",
    "    else:  # compute results for only one fold\n",
    "\n",
    "        cv_splits = list(skf.split(features, np.squeeze(y)))\n",
    "\n",
    "        train = cv_splits[args.folds][0]\n",
    "        test = cv_splits[args.folds][1]\n",
    "\n",
    "        val = test\n",
    "\n",
    "        scores_acc, scores_auc, scores_lin, scores_auc_lin, fold_size = train_fold(train, test, val, graph, features, y,\n",
    "                                                         y_data, params, subject_IDs)\n",
    "\n",
    "        print('overall linear accuracy %f' + str(np.sum(scores_lin) * 1. / fold_size))\n",
    "        print('overall linear AUC %f' + str(np.mean(scores_auc_lin)))\n",
    "        print('overall accuracy %f' + str(np.sum(scores_acc) * 1. / fold_size))\n",
    "        print('overall AUC %f' + str(np.mean(scores_auc)))\n",
    "\n",
    "    if args.save == 1:\n",
    "        result_name = 'ABIDE_classification.mat'\n",
    "        sio.savemat('./results/' + result_name ,\n",
    "                    {'lin': scores_lin, 'lin_auc': scores_auc_lin,\n",
    "                     'acc': scores_acc, 'auc': scores_auc, 'folds': fold_size})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\PFA\\population-gcn2\\ABIDEParser.py:224: RuntimeWarning: divide by zero encountered in arctanh\n",
      "  norm_networks = [np.arctanh(mat) for mat in all_networks]\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ABIDEParser as Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading timeseries file C:/Users/ousem/nilearn_data\\ABIDE_pcp/cpac/filt_noglobal\\50003\\Pitt_0050003_rois_ho.1D\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[ -4.671351,  -6.610331,   6.539966, ...,  23.815056,  29.358294,\n",
       "          12.700474],\n",
       "        [-12.931799, -10.146871,  -3.924838, ...,  14.159346,  29.569822,\n",
       "          10.523634],\n",
       "        [-20.238678, -11.706947, -22.075667, ...,  -4.235137,  12.97839 ,\n",
       "          -1.103658],\n",
       "        ...,\n",
       "        [-28.533656, -20.208223, -45.772988, ..., -28.597764, -96.24279 ,\n",
       "         -32.141592],\n",
       "        [-13.366269, -10.901036, -22.441715, ..., -10.383815, -32.775622,\n",
       "          -7.994821],\n",
       "        [ -1.730997,  -2.723371,  -1.761741, ...,   3.554802,  15.454366,\n",
       "          10.122492]])]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_series = Reader.get_timeseries(['50003'], 'ho')\n",
    "time_series"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "population-gcn2-YZL6I4UQ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
